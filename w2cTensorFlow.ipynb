{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f39552d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import MeCab\n",
    "import jieba\n",
    "import numpy as np\n",
    "from six.moves import xrange\n",
    "import tensorflow.compat.v1 as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1cfef4",
   "metadata": {},
   "source": [
    "# 停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba2c0cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142\n"
     ]
    }
   ],
   "source": [
    "stop_words = []\n",
    "with open('ja_stopwords.txt','r',encoding='utf-8')as f_stopwords:\n",
    "    for line in f_stopwords:\n",
    "        line  =line.replace('\\r','').replace('\\n','').strip()\n",
    "        stop_words.append(line)\n",
    "\n",
    "stop_words = set(stop_words)  #去重\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb58d3a0",
   "metadata": {},
   "source": [
    "# 预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddec3f7a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ノルウェー', '森', '村上', '春樹', '第', '一', '章', '僕', '三十', '七', '歳', 'ボーイング', 'シート', '座っ', '巨大', '飛行', '機', 'ぶ厚い', '雨雲', 'くぐり抜け', '降下', 'ハンブルク', '空港', '着陸', 'しよう', '十', '一', '月', '冷ややか', '雨']\n",
      "76346\n",
      "8875\n",
      "[['ノルウェー', 'の', '森'], ['村上', '春樹'], ['第', '一', '章'], ['僕', 'は', '三十', '七', '歳', 'で', 'その', 'とき', 'ボーイング', 'の', 'シート', 'に', '座っ', 'て', 'い', 'た', 'その', '巨大', 'な', '飛行', '機', 'は', 'ぶ厚い', '雨雲', 'を', 'くぐり抜け', 'て', '降下', 'し', 'ハンブルク', '空港', 'に', '着陸', 'しよう', 'と', 'し', 'て', 'いる', 'ところ', 'だっ', 'た', '十', '一', '月', 'の', '冷ややか', 'な', '雨', 'が', '大地', 'を', '暗く', '染め', '雨合羽', 'を', '着', 'た', '整備', '工', 'たち', 'や', 'のっぺり', 'と', 'し', 'た', '空港', 'ビル', 'の', '上', 'に', '立っ', 'た', '旗', 'や', 'の', '広告', '板', 'や', 'そんな', '何', 'も', 'か', 'も', 'を', 'フランドル', '派', 'の', '陰うつ', 'な', '絵', 'の', '背景', 'の', 'よう', 'に', '見せ', 'て', 'い', 'た', 'やれ', 'やれ', 'また', 'ドイツ', 'か', 'と', '僕', 'は', '思っ', 'た'], ['飛行', '機', 'が', '着地', 'を', '完了', 'する', 'と', '禁煙', 'の', 'サイン', 'が', '消え', '天井', 'の', 'スピーカー', 'から', '小さな', '音', 'で', 'が', '流れ', 'はじめ', 'た', 'それ', 'は', 'どこ', 'か', 'の', 'オーケストラ', 'が', '甘く', '演奏', 'する', 'ビートルズ', 'の', 'ノルウェイ', 'の', '森', 'だっ', 'た', 'そして', 'その', 'メロディー', 'は', 'いつ', 'も', 'の', 'よう', 'に', '僕', 'を', '混乱', 'さ', 'せ', 'た', 'いや', 'いつ', 'も', 'と', 'は', '比べもの', 'に', 'なら', 'ない', 'くらい', '激しく', '僕', 'を', '混乱', 'さ', 'せ', '揺り動かし', 'た'], ['僕', 'は', '頭', 'が', 'はりさけ', 'て', 'しまわ', 'ない', 'よう', 'に', '身', 'を', 'かがめ', 'て', '両手', 'で', '顔', 'を', '覆い', 'その', 'まま', 'じっと', 'し', 'て', 'い', 'た', 'やがて', 'ドイツ', '人', 'の', 'スチュワーデス', 'が', 'やっ', 'て', 'き', 'て', '気分', 'が', 'わるい', 'の', 'か', 'と', '英語', 'で', '訊い', 'た', '大丈夫', '少し', '目まい', 'が', 'し', 'た', 'だけ', 'だ', 'と', '僕', 'は', '答え', 'た'], ['本当', 'に', '大丈夫'], ['大丈夫', 'です', 'ありがとう', 'と', '僕', 'は', '言っ', 'た', 'スチュワーデス', 'は', 'にっこり', 'と', '笑っ', 'て', '行っ', 'て', 'しまい', '音楽', 'は', 'ビリー', '・', 'ジョエル', 'の', '曲', 'に', '変っ', 'た', '僕', 'は', '顔', 'を', '上げ', 'て', '北海', 'の', '上空', 'に', '浮かん', 'だ', '暗い', '雲', 'を', '眺め', '自分', 'が', 'これ', 'まで', 'の', '人生', 'の', '過程', 'で', '失っ', 'て', 'き', 'た', '多く', 'の', 'もの', 'の', 'こと', 'を', '考え', 'た', '失わ', 'れ', 'た', '時間', '死に', 'あるいは', '去っ', 'て', 'いっ', 'た', '人', 'もう', '戻る', 'こと', 'の', 'ない', '想い'], ['飛行', '機', 'が', '完全', 'に', 'ストップ', 'し', 'て', '人', 'が', 'シート', 'ベルト', 'を', '外し', '物入れ', 'の', '中', 'から', 'バッグ', 'やら', '上着', 'やら', 'を', 'とりだし', '始める', 'まで', '僕', 'は', 'ずっと', 'あの', '草原', 'の', '中', 'に', 'い', 'た', '僕', 'は', '草', 'の', '匂い', 'を', 'かぎ', '肌', 'に', '風', 'を', '感じ', '鳥', 'の', '声', 'を', '聴い', 'た', 'それ', 'は', '一', '九', '六', '九', '年', 'の', '秋', 'で', '僕', 'は', 'もう', 'すぐ', '二十', '歳', 'に', 'なろう', 'と', 'し', 'て', 'い', 'た'], ['前', 'と', '同じ', 'スチュワーデス', 'が', 'やっ', 'て', 'き', 'て', '僕', 'の', '隣り', 'に', '腰', 'を', '下ろし', 'もう', '大丈夫', 'か', 'と', '訊ね', 'た']]\n"
     ]
    }
   ],
   "source": [
    "tagger_owakati = MeCab.Tagger('-Owakati')\n",
    "raw_word_list = []\n",
    "sentence_owakati = []\n",
    "rules = u'([\\u3040-\\u9faf]+)'\n",
    "pattern = re.compile(rules)#正则\n",
    "count = 0\n",
    "#f_writer = open('挪威的森林_pro.txt','w',encoding = 'utf-8')\n",
    "with open('挪威的森林.txt','r',encoding = 'utf-8')as f_reader:\n",
    "    lines = f_reader\n",
    "    for line in lines:\n",
    "        line = line.replace('\\r',' ').replace('\\n',' ').strip()\n",
    "        if line == '' or line is None:\n",
    "            continue\n",
    "        line = ''.join(tagger_owakati.parse(line))\n",
    "        seg_list = pattern.findall(line)\n",
    "        sentence_owakati.append(seg_list)\n",
    "        word_list = []\n",
    "        for word in seg_list:\n",
    "            if word not in stop_words:\n",
    "                word_list.append(word)\n",
    "        if len(word_list)>0:\n",
    "            raw_word_list.extend(word_list)\n",
    "            line = ' '.join(word_list)\n",
    "  #          f_writer.write(line+'\\n')\n",
    " #           f_writer.flush()\n",
    "\n",
    "#f_writer.close()\n",
    "print(raw_word_list[:30])\n",
    "print(len(raw_word_list))\n",
    "print(len(set(raw_word_list)))\n",
    "print(sentence_owakati[:10])\n",
    "vocabulary_size = len(set(raw_word_list))\n",
    "#     for line in f_reader:\n",
    "#         line = line.replace('\\r','').replace('\\n','').strip()\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8234bb",
   "metadata": {},
   "source": [
    "# 文本编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20379b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'あなた'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = raw_word_list\n",
    "count = [['UNK','-1']]\n",
    "collections.Counter(words).most_common(20)\n",
    "count.extend(collections.Counter(words).most_common(vocabulary_size))\n",
    "#print(count)\n",
    "\n",
    "dictionary = dict()   #创建一个按照出现个数逆序编码 用字符对应整形\n",
    "for word , _ in count:\n",
    "    dictionary[word] = len(dictionary) \n",
    "\n",
    "#print(dictionary)\n",
    "\n",
    "data = list()\n",
    "unk_count = 0\n",
    "for word in words:       #用字典编码表示真个文本 存入data列表\n",
    "    if word in dictionary:\n",
    "        index = dictionary[word]\n",
    "    else:\n",
    "        index = 0      #如果出现未知则用0标识\n",
    "        unk_count +=1\n",
    "    data.append(index)\n",
    "\n",
    "count[0][1] = unk_count #最后将未知的个数输入\n",
    "\n",
    "#颠倒键值对 用整数查找字符\n",
    "reverse_dictionary = dict(zip(dictionary.values(),dictionary.keys()))\n",
    "reverse_dictionary[11]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a875b438",
   "metadata": {},
   "source": [
    "# 批量处理生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5000c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_index = 0\n",
    "# batch_size = 8\n",
    "# num_skips = 2  # 同时取词的个数\n",
    "# skip_window = 1 #窗口大小\n",
    "# batch = np.ndarray(shape = (batch_size),dtype = np.int32)\n",
    "# labels = np.ndarray(shape = (batch_size,1),dtype = np.int32)\n",
    "# span = 2*skip_window +1\n",
    "# buffer = collections.deque(maxlen = span)\n",
    "# for _ in range(span):\n",
    "#     buffer.append(data[data_index])\n",
    "#     data_index = (data_index +1) %len(data)\n",
    "# for i in range(batch_size // num_skips):\n",
    "#     target = skip_window\n",
    "#     targets_to_avoid = [skip_window]\n",
    "#     for j in range(num_skips):\n",
    "#         while target in targets_to_avoid:\n",
    "#             target = random.randint(0,span-1)\n",
    "#         targets_to_avoid.append(target)\n",
    "#         batch[i * num_skips +j ] = buffer[skip_window]\n",
    "#         labels[ i * num_skips +j] = buffer[target]\n",
    "#     buffer.append(data[data_index])\n",
    "#     data_index = (data_index +1)% len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ed93d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(8):\n",
    "#     print (batch[i],reverse_dictionary[batch[i]],'-->',labels[i,0],reverse_dictionary[labels[i,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8c21ad",
   "metadata": {},
   "source": [
    "# 封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f21c75f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size,num_skips,skip_window):\n",
    "    global data_index\n",
    "    batch = np.ndarray(shape = (batch_size),dtype = np.int32)\n",
    "    labels = np.ndarray(shape = (batch_size,1),dtype = np.int32)\n",
    "    span = 2*skip_window +1\n",
    "    buffer = collections.deque(maxlen = span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index +1) %len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0,span-1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips +j ] = buffer[skip_window]\n",
    "            labels[ i * num_skips +j] = buffer[target]\n",
    "        buffer.append(data[data_index]) \n",
    "        data_index = (data_index +1)% len(data)\n",
    "    return batch,labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be75da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch,labels = generate_batch(batch_size = 128, num_skips = 4, skip_window =2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53830d8",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "for i in range(120):\n",
    "    print (batch[i],reverse_dictionary[batch[i]],'-->',labels[i,0],reverse_dictionary[labels[i,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e470e5",
   "metadata": {},
   "source": [
    "# skipgram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "664ca2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 300\n",
    "skip_window = 2\n",
    "num_skips = 4\n",
    "valid_window = 100\n",
    "num_sample = 64\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "#校验集Z\n",
    "valid_word = ['緑','僕','永沢']\n",
    "valid_examples = [dictionary[li] for li in valid_word]\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    #输入\n",
    "    train_inputs = tf.placeholder(tf.int32,shape=[batch_size]) #训练集\n",
    "    train_labels = tf.placeholder(tf.int32,shape=[batch_size,1])#labels \n",
    "    valid_dataset = tf.constant(valid_examples,dtype=tf.int32)#检验\n",
    "    with tf.device('/cpu:0'):\n",
    "        embeddings = tf.Variable(tf.random_uniform([vocabulary_size,embedding_size],-1.0,1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings,train_inputs)\n",
    "        \n",
    "        nce_weight = tf.Variable(tf.truncated_normal([vocabulary_size,embedding_size],\n",
    "                                                     stddev=1.0/math.sqrt(embedding_size)))\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]),\n",
    "                                 dtype = tf.float32)\n",
    "        \n",
    "        neg_loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight,\n",
    "                                                 biases = nce_biases,\n",
    "                                                 inputs = embed,\n",
    "                                                 labels= train_labels,\n",
    "                                                num_sampled = num_sample,\n",
    "                                                num_classes = vocabulary_size))\n",
    "    \n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(neg_loss)\n",
    "        \n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings),1,keep_dims=True))\n",
    "        normalized_embeddings = embeddings / norm\n",
    "        \n",
    "        valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,valid_dataset)\n",
    "        similarity = tf.matmul(valid_embeddings,normalized_embeddings,transpose_b=True)\n",
    "        init = tf.global_variables_initializer()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fda7b5",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cfe9a00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======initialization complete======\n",
      "average loss is among 0 is 216.22235107421875\n",
      "[1185 1042 7923 2644 1643 7157 2942 2881 4938 6461]\n",
      "('the top 10 nearest words to', '緑', 'are:') 電気 閉め 鰻屋 無力 一人ぼっち バッチリ 薄 動詞 すえ しま\n",
      "[ 991 4722 7204  564 8859 8826 3023 4118 6540 6548]\n",
      "('the top 10 nearest words to', '僕', 'are:') おなか 過ぎ去っ もがく 撫で 興 恋愛 ぜんぜん 食料 はずし 殺し\n",
      "[7915 1542 7098  695 7661 7376 6058  869 7103   71]\n",
      "('the top 10 nearest words to', '永沢', 'are:') うーん 一杯 幾 ニ 高熱 ねっ 地底 飲ま 本能 いっ\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'embedding_lookup' defined at (most recent call last):\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\traitlets\\config\\application.py\", line 985, in launch_instance\n      app.start()\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Temp\\ipykernel_20600\\656130858.py\", line 22, in <module>\n      embed = tf.nn.embedding_lookup(embeddings,train_inputs)\nNode: 'embedding_lookup'\nindices[28] = 8875 is not in [0, 8875)\n\t [[{{node embedding_lookup}}]]\n\nOriginal stack trace for 'embedding_lookup':\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\traitlets\\config\\application.py\", line 985, in launch_instance\n    app.start()\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n    self.io_loop.start()\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 600, in run_forever\n    self._run_once()\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1896, in _run_once\n    handle._run()\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n    await self.process_one()\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n    await dispatch(*args)\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n    await result\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n    reply_content = await reply_content\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n    res = shell.run_cell(\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in run_cell\n    result = self._run_cell(\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2995, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3194, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3373, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3433, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Temp\\ipykernel_20600\\656130858.py\", line 22, in <module>\n    embed = tf.nn.embedding_lookup(embeddings,train_inputs)\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 150, in error_handler\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 1176, in op_dispatch_handler\n    return dispatch_target(*args, **kwargs)\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\embedding_ops.py\", line 330, in embedding_lookup\n    return _embedding_lookup_and_transform(\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\embedding_ops.py\", line 144, in _embedding_lookup_and_transform\n    array_ops.gather(params[0], ids, name=name), ids, max_norm)\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 150, in error_handler\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 1176, in op_dispatch_handler\n    return dispatch_target(*args, **kwargs)\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 561, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 5306, in gather\n    return params.sparse_read(indices, name=name)\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 752, in sparse_read\n    value = gen_resource_variable_ops.resource_gather(\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py\", line 688, in resource_gather\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 795, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3798, in _create_op_internal\n    ret = Operation(\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1378\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1378\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1361\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extend_graph()\n\u001b[1;32m-> 1361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_tf_sessionrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1362\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1454\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_tf_sessionrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[0;32m   1453\u001b[0m                         run_metadata):\n\u001b[1;32m-> 1454\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRun_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1455\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1456\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: indices[28] = 8875 is not in [0, 8875)\n\t [[{{node embedding_lookup}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m xrange(num_steps):\n\u001b[0;32m      9\u001b[0m     batch_inputs,batch_labels \u001b[38;5;241m=\u001b[39m generate_batch(batch_size,\n\u001b[0;32m     10\u001b[0m                                                num_skips,\n\u001b[0;32m     11\u001b[0m                                                skip_window)\n\u001b[1;32m---> 12\u001b[0m     _, loss \u001b[38;5;241m=\u001b[39m \u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mneg_loss\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_inputs\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_labels\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     avg_loss \u001b[38;5;241m=\u001b[39m avg_loss \u001b[38;5;241m+\u001b[39m loss\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    965\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    970\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[0;32m    971\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1191\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;66;03m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[0;32m   1189\u001b[0m \u001b[38;5;66;03m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_fetches \u001b[38;5;129;01mor\u001b[39;00m final_targets \u001b[38;5;129;01mor\u001b[39;00m (handle \u001b[38;5;129;01mand\u001b[39;00m feed_dict_tensor):\n\u001b[1;32m-> 1191\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_fetches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfeed_dict_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1194\u001b[0m   results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1371\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1368\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1371\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_run_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1397\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124monly supports NHWC tensor format\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m message:\n\u001b[0;32m   1393\u001b[0m   message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mA possible workaround: Try disabling Grappler optimizer\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1394\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mby modifying the config for creating the session eg.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1395\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124msession_config.graph_options.rewrite_options.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1396\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisable_meta_optimizer = True\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1397\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(node_def, op, message)\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'embedding_lookup' defined at (most recent call last):\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\traitlets\\config\\application.py\", line 985, in launch_instance\n      app.start()\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\fakenews\\AppData\\Local\\Temp\\ipykernel_20600\\656130858.py\", line 22, in <module>\n      embed = tf.nn.embedding_lookup(embeddings,train_inputs)\nNode: 'embedding_lookup'\nindices[28] = 8875 is not in [0, 8875)\n\t [[{{node embedding_lookup}}]]\n\nOriginal stack trace for 'embedding_lookup':\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\traitlets\\config\\application.py\", line 985, in launch_instance\n    app.start()\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n    self.io_loop.start()\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 600, in run_forever\n    self._run_once()\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1896, in _run_once\n    handle._run()\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n    await self.process_one()\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n    await dispatch(*args)\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n    await result\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n    reply_content = await reply_content\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n    res = shell.run_cell(\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in run_cell\n    result = self._run_cell(\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2995, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3194, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3373, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3433, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Temp\\ipykernel_20600\\656130858.py\", line 22, in <module>\n    embed = tf.nn.embedding_lookup(embeddings,train_inputs)\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 150, in error_handler\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 1176, in op_dispatch_handler\n    return dispatch_target(*args, **kwargs)\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\embedding_ops.py\", line 330, in embedding_lookup\n    return _embedding_lookup_and_transform(\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\embedding_ops.py\", line 144, in _embedding_lookup_and_transform\n    array_ops.gather(params[0], ids, name=name), ids, max_norm)\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 150, in error_handler\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 1176, in op_dispatch_handler\n    return dispatch_target(*args, **kwargs)\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 561, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 5306, in gather\n    return params.sparse_read(indices, name=name)\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 752, in sparse_read\n    value = gen_resource_variable_ops.resource_gather(\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py\", line 688, in resource_gather\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 795, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"C:\\Users\\fakenews\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3798, in _create_op_internal\n    ret = Operation(\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20000\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    init.run()\n",
    "    print('======initialization complete======')\n",
    "    \n",
    "    avg_loss = 0\n",
    "    \n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs,batch_labels = generate_batch(batch_size,\n",
    "                                                   num_skips,\n",
    "                                                   skip_window)\n",
    "        _, loss = sess.run([optimizer,neg_loss],feed_dict = {\n",
    "            train_inputs:batch_inputs,\n",
    "            train_labels:batch_labels\n",
    "        })\n",
    "        avg_loss = avg_loss + loss\n",
    "        \n",
    "        if step % 5000 == 0:\n",
    "            if step > 0:\n",
    "                avg_loss = avg_loss / 5000\n",
    "            print(\"average loss is among\",step,\"is\",avg_loss)\n",
    "            \n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in xrange(len(valid_word)):\n",
    "                val_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 10\n",
    "                nearest = (sim[i,:].argsort()[:top_k])\n",
    "                print(nearest)\n",
    "                sim_str = 'the top 10 nearest words to',val_word,'are:'\n",
    "                for k in xrange(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    sim_str = '%s %s' %(sim_str,close_word)\n",
    "                print(sim_str)\n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
