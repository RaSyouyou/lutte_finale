{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da07288f",
   "metadata": {},
   "source": [
    "# Gensim Word2vec参数\n",
    "1）sentences:我们要分析的语料 可以是一个列表 或者从文件中遍历读出<br/>\n",
    "2）vector_size：词向量的维度，默认值是100.这个维度的取值一般根据我们的预料的大小相关，如果是不大的语料，比如小于100m的文本语料，则使用默认值即可。如果是超大的语料增大维度.<br/>\n",
    "3）window：词向量上下文的距离。默认值是5 一般在[5，10]<br/>\n",
    "4）sg：两个模型选择： 0则是cbow模型，默认是0。<br/>\n",
    "               1则是skip—gram。 <br/>\n",
    "5）hs：两个解法选择： 0为Negative Sampling，默认是0。<br/>\n",
    "               1再负采样个数negative大于0时，采用Hierarchial Softmax<br/>\n",
    "6）negative：使用Negative Sampling时的个数 默认是5 一般再[3,10]<br/>\n",
    "7）cbow_mean :仅用于CBOW 在做投影的时候，为0， <br/>\n",
    "8) min_count：需要计算词向量的最小词频。默认是5 对于小语料可以降低这个值<br/>\n",
    "9)epochs：随机梯度下降迭代的最大次数，默认是5.对于大语料，可以增大这个值<br/>\n",
    "10)alpha：在随机梯度下降法中迭代的初始补偿 默认是0.025<br/>\n",
    "11)min_alpha:<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f94c269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import re\n",
    "from gensim.models import word2vec\n",
    "from matplotlib import pyplot\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07994a56",
   "metadata": {},
   "source": [
    "# 停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22a010fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142\n"
     ]
    }
   ],
   "source": [
    "stop_words = []\n",
    "with open('ja_stopwords.txt','r',encoding='utf-8')as f_stopwords:\n",
    "    for line in f_stopwords:\n",
    "        line  =line.replace('\\r','').replace('\\n','').strip()\n",
    "        stop_words.append(line)\n",
    "\n",
    "stop_words = set(stop_words)  #去重\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba177a32",
   "metadata": {},
   "source": [
    "# 文本预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ba07708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = []\n",
    "# rules = u'([\\u3040-\\u9faf]+)'\n",
    "# pattern = re.complie(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c667075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ノルウェー', '森'], ['村上', '春樹'], ['第', '一', '章'], ['僕', '三十', '七', '歳', 'ボーイング', 'シート', '座っ', '巨大', '飛行', '機', 'ぶ厚い', '雨雲', 'くぐり抜け', '降下', 'ハンブルク', '空港', '着陸', 'しよう', '十', '一', '月', '冷ややか', '雨', '大地', '暗く', '染め', '雨合羽', '着', '整備', '工', 'のっぺり', '空港', 'ビル', '上', '立っ', '旗', '広告', '板', 'そんな', 'フランドル', '派', '陰うつ', '絵', '背景', '見せ', 'やれ', 'やれ', 'ドイツ', '僕', '思っ'], ['飛行', '機', '着地', '完了', '禁煙', 'サイン', '消え', '天井', 'スピーカー', '小さな', '音', '流れ', 'はじめ', 'オーケストラ', '甘く', '演奏', 'ビートルズ', 'ノルウェイ', '森', 'メロディー', 'いつ', '僕', '混乱', 'いや', 'いつ', '比べもの', 'くらい', '激しく', '僕', '混乱', '揺り動かし'], ['僕', '頭', 'はりさけ', 'しまわ', '身', 'かがめ', '両手', '顔', '覆い', 'まま', 'じっと', 'やがて', 'ドイツ', '人', 'スチュワーデス', 'やっ', '気分', 'わるい', '英語', '訊い', '大丈夫', '少し', '目まい', 'だけ', '僕', '答え'], ['本当', '大丈夫'], ['大丈夫', 'ありがとう', '僕', '言っ', 'スチュワーデス', 'にっこり', '笑っ', '行っ', 'しまい', '音楽', 'ビリー', '・', 'ジョエル', '曲', '変っ', '僕', '顔', '上げ', '北海', '上空', '浮かん', '暗い', '雲', '眺め', '自分', '人生', '過程', '失っ', '多く', '考え', '失わ', '時間', '死に', 'あるいは', '去っ', 'いっ', '人', 'もう', '戻る', '想い'], ['飛行', '機', '完全', 'ストップ', '人', 'シート', 'ベルト', '外し', '物入れ', '中', 'バッグ', 'やら', '上着', 'やら', 'とりだし', '始める', '僕', 'ずっと', '草原', '中', '僕', '草', '匂い', 'かぎ', '肌', '風', '感じ', '鳥', '声', '聴い', '一', '九', '六', '九', '年', '秋', '僕', 'もう', 'すぐ', '二十', '歳', 'なろう'], ['前', '同じ', 'スチュワーデス', 'やっ', '僕', '隣り', '腰', '下ろし', 'もう', '大丈夫', '訊ね']]\n"
     ]
    }
   ],
   "source": [
    "text = []\n",
    "with open('挪威的森林_pro.txt','r',encoding = 'utf-8')as f:\n",
    "    lines = f\n",
    "    for line in lines:\n",
    "        line = line.replace('\\n',' ').strip()\n",
    "        text.append(line.split(' '))\n",
    "        \n",
    "print(text[:10])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605a67d3",
   "metadata": {},
   "source": [
    "# 模型训练 查看特定词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d62910fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = word2vec.Word2Vec(text,window=3,vector_size=200,epochs=5)\n",
    "\n",
    "type(model.wv['緑'])\n",
    "#model.wv.index_to_key\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635e59c2",
   "metadata": {},
   "source": [
    "# 相近词语"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa6395cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "緑 0.9996391534805298\n",
      "直子 0.9996244311332703\n",
      "とても 0.9994981288909912\n",
      "笑っ 0.9994807243347168\n",
      "顔 0.9994432926177979\n",
      "声 0.999430239200592\n",
      "だけ 0.9994221329689026\n",
      "煙草 0.9994141459465027\n",
      "見 0.9994131922721863\n",
      "二人 0.9994045495986938\n",
      "み 0.9994038343429565\n",
      "しばらく 0.9994025230407715\n",
      "男 0.9993998408317566\n",
      "自分 0.9993836879730225\n",
      "ませ 0.9993786215782166\n"
     ]
    }
   ],
   "source": [
    "for e in model.wv.most_similar(positive=['僕'],topn = 15):\n",
    "    print(e[0],e[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c527856",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('挪威的森林_pro_window=3,vector_size=200,epochs=5.model')\n",
    "# model2 = word2vec.Word2Vec.load('挪威的森林_pro_window=3,vector_size=200,epochs=5.model') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ac85ac",
   "metadata": {},
   "source": [
    "# 计算两个词的相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e60f9017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9994653\n",
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gensim.models.word2vec.Word2Vec"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(dir(model))\n",
    "#print(dir(model.wv))\n",
    "sim_value = model.wv.similarity('手','足')\n",
    "print(sim_value)\n",
    "\n",
    "print(type(model.wv.index_to_key))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbfe49c",
   "metadata": {},
   "source": [
    "# 可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a2569e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a real number, not 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m X \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mkey_to_index\n\u001b[0;32m      2\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m pyplot\u001b[38;5;241m.\u001b[39mscatter(result[:,\u001b[38;5;241m0\u001b[39m],result[:,\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      6\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(model\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mindex_to_key)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 142\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    147\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    148\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:462\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \n\u001b[0;32m    441\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;124;03mC-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m--> 462\u001b[0m U, S, Vt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    463\u001b[0m U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhiten:\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;66;03m# X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:485\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    481\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPCA does not support sparse input. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTruncatedSVD for a possible alternative.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    483\u001b[0m     )\n\u001b[1;32m--> 485\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;66;03m# Handle n_components==None\u001b[39;00m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 535\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    536\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:877\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    875\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    876\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 877\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    880\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    881\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    182\u001b[0m     xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(array)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy.array_api\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'dict'"
     ]
    }
   ],
   "source": [
    "X = model.wv.key_to_index\n",
    "pca = PCA(n_components = 2)\n",
    "result = pca.fit_transform(X)\n",
    "\n",
    "pyplot.scatter(result[:,0],result[:,1])\n",
    "words = list(model.wv.index_to_key)\n",
    "for i,word in enumerate(words):\n",
    "    pyplot.annotate(word,xy=(result[i,0],result[i,1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c597d46d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
